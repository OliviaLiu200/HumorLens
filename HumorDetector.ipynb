{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.applications import efficientnet\n",
    "from keras.layers import TextVectorization\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "keras.utils.set_random_seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
    "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
    "!unzip -qq Flickr8k_Dataset.zip\n",
    "!unzip -qq Flickr8k_text.zip\n",
    "!rm Flickr8k_Dataset.zip Flickr8k_text.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dset = load_dataset(\"jmhessel/newyorker_caption_contest\", \"explanation\")\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ms coco dataset\n",
    "!git clone https://github.com/tylin/coco-caption.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the images\n",
    "IMAGES_PATH = \"Flicker8k_Dataset\"\n",
    "\n",
    "# Desired image dimensions\n",
    "IMAGE_SIZE = (299, 299)\n",
    "\n",
    "# Vocabulary size\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Fixed length allowed for any sequence\n",
    "SEQ_LENGTH = 25\n",
    "\n",
    "# Dimension for the image embeddings and token embeddings\n",
    "EMBED_DIM = 512\n",
    "\n",
    "# Per-layer units in the feed-forward network\n",
    "FF_DIM = 512\n",
    "\n",
    "# Other training parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_caption(caption):\n",
    "    caption = caption.lower().strip()\n",
    "    caption = re.sub(r'[^\\w\\s]', '', caption)\n",
    "    caption = re.sub('\\s+', ' ', caption)\n",
    "    return '<start> ' + caption + ' <end>'\n",
    "\n",
    "# Flickr8k\n",
    "IMAGES_PATH_FLICKR8K = \"Flicker8k_Dataset\"\n",
    "def load_flickr8k_data(filename):\n",
    "    with open(filename) as file:\n",
    "        data = file.readlines()\n",
    "    image_captions = {}\n",
    "    for line in data:\n",
    "        image_path, caption = line.strip().split('\\t')\n",
    "        image_path = os.path.join(IMAGES_PATH_FLICKR8K, image_path.split('#')[0])\n",
    "        if image_path in image_captions:\n",
    "            image_captions[image_path].append(preprocess_caption(caption))\n",
    "        else:\n",
    "            image_captions[image_path] = [preprocess_caption(caption)]\n",
    "    return image_captions\n",
    "\n",
    "flickr8k_captions = load_flickr8k_data(\"Flickr8k.token.txt\")\n",
    "\n",
    "# MS COCO\n",
    "BASE_PATH_COCO = '../input/coco-2017-dataset/coco2017'\n",
    "def load_coco_data(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)['annotations']\n",
    "    image_captions = {}\n",
    "    for item in data:\n",
    "        image_path = os.path.join(BASE_PATH_COCO, 'train2017', f'{item[\"image_id\"]:012d}.jpg')\n",
    "        caption = preprocess_caption(item['caption'])\n",
    "        if image_path in image_captions:\n",
    "            image_captions[image_path].append(caption)\n",
    "        else:\n",
    "            image_captions[image_path] = [caption]\n",
    "    return image_captions\n",
    "\n",
    "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
    "\n",
    "    all_images = list(caption_data.keys())\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(all_images)\n",
    "\n",
    "    train_size = int(len(caption_data) * train_size)\n",
    "\n",
    "    training_data = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
    "    }\n",
    "    validation_data = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
    "    }\n",
    "\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "coco_captions = load_coco_data(f'{BASE_PATH_COCO}/annotations/captions_train2017.json')\n",
    "\n",
    "# Combine the datasets\n",
    "combined_captions = {**flickr8k_captions, **coco_captions}\n",
    "\n",
    "# convert to a list of (image, caption) pairs for easier processing later\n",
    "combined_data = [(img, cap) for img, caps in combined_captions.items() for cap in caps]\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, valid_data = train_val_split(combined_data)\n",
    "print(\"Number of training samples: \", len(train_data))\n",
    "print(\"Number of validation samples: \", len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "strip_chars = strip_chars.replace(\"<\", \"\")\n",
    "strip_chars = strip_chars.replace(\">\", \"\")\n",
    "\n",
    "vectorization = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQ_LENGTH,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "vectorization.adapt(text_data)\n",
    "\n",
    "# Data augmentation for image data\n",
    "image_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomContrast(0.3),\n",
    "    ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
